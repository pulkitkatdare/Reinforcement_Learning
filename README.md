# Reinforcement learning Algorithms(RL)
<p>These were the codes that I wrote while undergoing David Silver's online course on RL. Many of these codes that I found were a part of a tutorial series that I found on the internet. These tutorials are roughly divided into five parts based on type of RL algorithms that I was pursuing</p>

- [Dynamic Programming](DP): These come under the model-based RL algorithms, wherein you have a complete understanding as to how the environment behaves. This module primarily contains three type of problems [Policy_Evaluation.py](DP/Policy_Evaluation.py),  [Policy_Iteration.py](DP/Policy_Iteration.py),  [Value_Iteration.py](DP/Value_Iteration.py)
- [Function Approximations](FA): It is not always possible to have prior information about the state-spaces. Also, it is also not always possible to perform value iterations due to the curse of dimensionality. This is the essential motivation behind Function approximations, wherein a function approximator(generally a neural net) is used to approximate the value function or Q-function  to a reasonable degree. In [Q-learning.py](FA/Q-learning.py), we learn a Q-function to run the MountainCar environment in openai gym. The Q-function is being updated using the Bellman equation
- [Cross Entropy Method](CEM): One of the first approaches to function approximation based policy optimisation. It begins with initialising the parameters of the policy function from a distribution(in this case gaussian) with a given mean and variance and we keep on updating this mean and variance based on the mean and variance of the data set which performs better over this dataset. 
- [Policy Gradient](PG): Every Reinforcement learning algorithm tries to maximize the expected reward. Since, in many of the RL problems the norm is to use a function approximators. It makes much more sense to update the weights of this policy function in direction of its gradient with respect to the Expected reward. In Policy gradient method, we use a popular mathematical result to evaluate policy gradient we make use of the [policy gradient theorem](https://homes.cs.washington.edu/~todorov/courses/amath579/reading/PolicyGradient.pdf). 
- [Deep-Q-learning](DQN): In this repository we implement deep-Q-learning to play atari breakout game. It has two important qualities, a replay buffer to generate sample and the use of deep neural net to determine the Q-function value from raw pixel data. Since, the Bellman equation is being used to update the Q-function that is being generates the target the learning is unstable. To improve the stability it was suggested to use target-Q function to calculate the target, this greatly improves stability and the rate of learning. This algorithms is also popularly called as double Q-learning. 